# 介绍
DQN（deep Q netwrok）。使用神经网络来计算Q函数。

**解决什么问题？**

答：基于table的方法会把所有的state或者action的价值函数全部列出来，如果空间特别大，会消耗特别多的存储空间，同时计算出每个价值也需要大量的时间。除此之外，对于状态空间连续，则存在无限多个状态，基于table的方法无法进行存储。

与基于table方法不同，DQN使用的是神经网络的方式来存储和计算Q。

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1767070347391-86aac547-5ac3-4f52-8d43-5050ccff4e3d.png)

DQN借鉴了**value function approximation**和**神经网络技术**，并采用`**目标网络**`和`**经验回放**`的方法进行网络的训练。

> 补充，以下都是基于value based 的method，即通过计算出value function，使用贪心算法生成对应的policy，指导agent与环境交互。
>

# state value function
我们学习的不是policy（`**actor**`），而是`**critic**`**（衡量对应state的价值）**。求解state value function的方法通常为：TD算法和蒙特卡洛的方式，两者的区别：

+ TD算法每次行动一步，都会更新上一步状态的value；
+ 蒙特卡洛必须要等agent和环境完成全部的交互，才能进行更新，因此在实际中我们很少使用这个蒙特卡洛进行计算。
+ 蒙特卡洛的方差特别大，TD的方差较小。

# Q value function
`**action value**`。描述在某个状态，采取某个动作能带来的价值。与state value function一样，求解方法有：TD算法和蒙特卡洛的方法进行。

我们会采用exploration的算法完成对应的学习，不过在后期可以慢慢降低exploration的大小，让模型policy进入完全贪心算法的逻辑中。

# DQN
DQN是 value based 算法。该算法用神经网络的方式计算Q，因此对于它而言，需要确定目标函数，和采样方法。

`**基于TD网络的目标函数**`：通过Q的定义，输入一个s和a，他的输出应该是下一个 s' 的state value 加上对应的 reward。reward可以采样获得，下一个状态s' 也可以采样获得。关键在于s' 的state value如何获得。我们根据state value的定义，选择贪心算法，即在s' 能获得最大的action value作为其state value，因此可以将s' 再次输入DQN网络，得到对应的state value，然后结合reward作为前一个Q（s，a）的target。

**<font style="color:#DF2A3F;">问题</font>**：如果计算Q的网络和计算target的网络采取同一个会有什么问题？

如果采取同一个网络，则在优化Q时，也会使得计算target发生变化，两个都发发生变化会使训练不稳定，优化轨迹难以理解。

**<font style="color:#74B602;">解决方案</font>**：分别使用两个网络，一个用于计算Q，一个用于计算Q target。定期使用Q的参数更新Q target，完成训练。



`**采样方法**`：根据前面的描述，我们发现对于训练Q网络而言，需要的仅仅是(s, a, r, s'）,而不需要具体的action value、state value，因此我们可以采用off line policy的学习方式进行学习。为此，提出了一个`**经验回放**`的技巧。即将当前策略与环境生成的交互数据放入缓冲区，模型通过在缓冲区采样，获取对应的训练数据。

**好处**：在模型训练中，模型参数的优化耗时较小，但是agent和env交互产生数据的时间较长，使用经验回放可以减少生成数据的时间，加快模型训练。



`**基于蒙特卡洛的目标函数**`：这个方法即为最简单的逻辑回归，通过输入一个state action，然后神经网络输出一个prediction。Target是通过agent与环境进行大量交互获取。（该方法不怎么使用，因为他需要agent与env进行大量的交互才行）。



# 问题
## 为什么在深度Q网络中采用价值函数近似的表示方法？
+ 基于table的方法针对状态空间有限时，可以满足价值函数的表达和存储，但是针对状态连续，则无法进行表达。
+ 基于神经网络近似的方法，泛化性更好
+ 基于神经网络近似的方法存储开销更小。

## critic的输出通常与几个值有关？
critic的目标是评估某个policy在某个state的价值，因此与这两个值有关

## 我们通常如何衡量$ V_\pi(s)？ $其优缺点呢？
蒙特卡洛、TD算法

蒙特卡洛：通过agent和env大量的交互获取数据，估计对应的value。缺点是：必须要有足够的采样数据，如果采样数据没有覆盖所有的state，则无法训练对应的state。在采样时需要等待，时间开销大。方差也大（但无偏估计）。

TD算法：agent每次和env进行一次交互，即对state进行一次更新。优点：不需要等待agent和env完成所有交互才进行学习；方差小，但是，求出来的值不一定准确（有偏估计）。

## 基于蒙特卡洛的网络方法，我们怎么训练模型？
基于蒙特卡洛的方法和有监督学习中的逻辑回归相似。我们通过给神经网络输入s和a，获取对应的prediction，然后agent与env进行大量交互获得的最终结果作为target，让prediction去你和target。

## 基于TD的网络方法，我们如何训练模型？
基于时序差分的方法核心在于：$ V_\pi(s_t) = V_\pi(s_{t+1}) + r_t $。因此，我们将$ s_t $输入网络，会得到对应的输出$ V_\pi(s_t) $,输入$ s_{t+1} $得到$ V_\pi(s_{t+1}) $。我们希望$ V_\pi(s_{t+1})-V_\pi(s_t) $的值为$ r_t $。因此，这就是训练的优化目标，这就是损失函数。

具体的训练过程，会将生成V的网络和生成target的网络进行分开，防止同时训练啊导致target和v都在发生变化，训练不稳定。定期更新网络的参数。

## 使用经验回放的好处？
节省agent和env进行交互导致的大量时间开销。

提高采样数据的多样性。

## 在经验回放中，经验来自不同的policy，会有影响吗？
不会，因为我们采样的是一个经验，而不是一个轨迹，因此是不是off-line policy也没关系。

## DQN是什么？其两个关键性的技巧是什么？
基于神经网络的Q学习算法，结合了神经网络和价值函数近似技术。其通过目标网络和经验回放完成训练。

## 目标网络和经验回放是为了什么？
**目标网络**：是防止在训练过程中，更新价值网络参数导致目标也同步发生变化，使得模型训练不稳定的问题。

**经验回放**：讲agent的env进行交互的经验放入经验池，在经验池中进行采样。节约产生经验的时间。

## 不打破数据相关性，神经网络训练为什么不好？
神经网络中通常使用随机梯度下降（GD）。随机的意思是随机选择一些样本进行增量式的估计梯度。如果样本相关的，则意味着前后两个batch可能也是相关的，那么梯度就可能呈现相关性，后面的梯度估计可能会抵消前面的梯度估计量，从而使得训练难以拟合。



# DQN（连续动作）
前面介绍的都是针对离散动作，即动作可以全部枚举的情况。如果针对连续动作应该如何做呢？

## 对动作进行采样
对连续动作进行离散采样，带入计算，获取最大的Q值，采取对应的action。该方法最后的动作可能不是很准确。

## 梯度上升
将action作为参数，找一组最大的a，使得最大化Q函数。该方法训练量大

## 不使用DQN
使用DQN处理连续动作是非常麻烦的，因此可以考虑PPO和critic的方法结合。

