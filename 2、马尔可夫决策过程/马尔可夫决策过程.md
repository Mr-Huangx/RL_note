# 马尔可夫性质
在随机过程中，**马尔可夫性质（Markov property）**是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。



# Return和价值函数
**回报（return）**定义为episode的reward和。

**折扣回报（discounted return）**定义为对episode的reward进行折扣相加。

**状态价值函数（state value function）**定义为某个状态下所有`**Return**`的`**期望**`。

注：使用discounted return的原因是因为：有些马尔可夫过程带环，他不会终结。其次，如果奖励有价值，我们希望能立刻获得奖励。



# Bellman equation
`**state value function**`价值函数的计算公式。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1766638831799-870de129-6ad9-4376-962f-07954eb356ef.png)

求解贝尔曼公式的方法有两个：

+ **解析解**，采用矩阵求解方法进行；
+ **迭代的方式**。

注意观察：贝尔曼公式没有涉及到action！！！



# 马尔可夫决策过程
相较于马尔可夫奖励过程，马尔可夫决策过程多了决策（即动作），其他定义和马尔可夫奖励过程是相类似的。因此，状态转移也多了一个条件，即action。

$ p(s_{t+1}|s_t,a_t)=p(s_{t+1}|h_t,a_t) $

对于reward奖励函数，他多了一个action，变成了$ R(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a] $。当前状态以及采取的动作会决定agent在当前得到的奖励多少。

## 马尔可夫决策过程中的策略
策略定义了在某一个状态应该采取什么样的动作。知道状态，即可获取对应action的概率：

$ \pi(a|s) = p(a_t=a|s_t=s) $

## Q函数
动作价值函数`**action-value function**`。定义一个action可能得到的回报。

$ Q_\pi(s,a) = E_\pi[G_t|s_t=s,a_t=a] $

这里的期望是机遇策略函数的。所以需要求均值。可以仔细观察得到：

$ V_\pi(s)=\sum_{a\in A}\pi(a|s)Q_\pi(s,a) $

反之，Q也可以由直接reward和value function推导出来：

$ Q(s,a)=R(s,a) + y\sum_{s'\in S}p(s'|s,a)V(s') $

## Bellman expectation equation
我们可以把状态价值函数和 Q 函数拆解成两个部分：即时奖励和后续状态的折扣价值（discounted value of successor state）。 通过对状态价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程————**贝尔曼期望方程（Bellman expectation equation）**：

$ V_\pi(s) = E_\pi[r_{t+1} + yV_\pi(s_{t+1})|s_t=s] $

对于Q函数一样可以进行分解：

$ Q_\pi(s,a)=E_\pi[r_{t+1}+yQ_\pi(s_{t+1},a_{t+1})|s_t=s,a_t=a] $

贝尔曼期望方程定义了当前状态与未来状态之间的关联。



# Policy evaluation
`**策略评估**`。我们通过计算state value来评估马尔可夫决策过程中，策略的好坏。如果一个策略能使得每个state的state value相比其他policy都大，我们认为这个policy就是最优的策略（optimal policy）。



# 贝尔曼最优方程
当我们使用`**policy evaluation**`和`**policy improvement**`不断对`**policy**`进行迭代时，最终会让`**policy**`收敛为最优。如果再迭代过程中，我们采用`**argmax**`的方法，选择当前能获得最大价值的action，那最终收敛之后的policy，其对应的`**state value和action value会相同**`。此时，贝尔曼公式的表达式称之为贝尔曼最优公式。

$ Q_\pi(s,\pi'(s))=max_{a\in A}Q_\pi(s,a)=Q_\pi(s,\pi(s))=V_\pi(s) $

贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。



# Policy iteration和Value iteration
## Policy iteration
通过不断反复：policy evaluation和policy improvement来完成策略的迭代，让策略达到最优。

## Value iteration
通过不断使用贝尔曼最优方程进行迭代，求得state value的最终解，找到最佳的action value，然后生成最佳策略policy。



# 习题
## return为什么要采用折扣因子？
+ 马尔可夫过程存在环
+ 我们希望及时奖励，而不是延时奖励
+ 可以控制agent对短期和长期奖励的关注程度。

## 计算贝尔曼公式的方法有哪些？
+ 蒙特卡洛
+ 动态规划：不断进行iteration的方式进行。
+ TD方法

## 寻找最优策略的方法有哪些？
+ value iteration
+ policy iteration

## 马尔可夫过程是什么？马尔可夫决策过程是什么？马尔可夫最重要的性质是什么？
马尔可夫过程是一个二元组：<S , P>，S为状态集合，P为状态转移函数；

马尔可夫决策过程是一个五元组：<S , P, A , R, y>， 其中R表示但从S到$ S' $能获得的奖励期望，y为折扣因子，A为动作集合。 

马尔可夫最重要的性质：马尔可夫性质，即未来的状态只于当前状态有关，与过去无关。

## 如何求解马尔可夫决策过程？
求解马尔可夫决策过程，即求解贝尔曼方程：

+ 矩阵求解（计算复杂，且困难）
+ 迭代法（value iteration）
+ 蒙特卡洛
+ TD方法

## 如果数据流不具备马尔可夫性质怎么办？
首先，马尔可夫性质指的是未来的状态只与当前状态相关，与过去的状态无关。如果不满足马尔可夫性质，说明未来的状态和过去的状态有关，如果仅考虑当前状态来求解，就会导致决策泛化性问题。解决这个问题，可以使用RNN（attention）这种，对历史信息建模，使得当前state包含历史信息即可。

## 为什么最优state function和最优policy等价？
因为policy evaluation也是通过state value进行评价的。如果state value达到最大，此时的policy我们称为optimal policy。此时的state value我们称为optimal value function。

