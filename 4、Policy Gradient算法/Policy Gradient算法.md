# Policy Gradient算法
简单来讲，

+ 使用神经网络的方式，输入当前状态s，然后输出action的分布。
+ 定义obejective function，通常采用reward的期望，来衡量policy好坏
+ 使用梯度上升的方式优化神经网络。

具体而言，就是如何设计objective function ，如何采样的问题。

# 重要性采样
简单来讲，就是使用另一个分布q，来采样获取p分布的期望。

**存在的问题**：

+ 两个分布不可以差距太大，否则如果采样次数不够，会出现较大偏差。
+ 由于涉及p/q的计算，如果q为零，p不可以为零，否则会没有定义。

**作用：**

+ 可以将online-policy转换为offline-policy。



# 近端策略优化PPO（proximal policy optimization）
问题：使用重要性采样，可以将online policy转换为off-line policy，但如果两个policy的分布相差太大，方差会很大，结果会不好。怎么避免呢？

答：PPO

`**具体做法**`：在原始的重要性采样的objective function上增加一项约束（KL散度），用于衡量p，q之间的差异，希望两者相差不多。（类似于正则化项）

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1767054017560-8d0807f3-f232-4d6f-8485-be26f86981ed.png)

`**注意**`：由于PPO加入了KL散度的约束，behavoir policy 和 target policy比较相近，因此可以认为是同一个策略，因此PPO也被认为是online-policy。



## KL散度
衡量两个分布之间的相似程度。可以理解为，给定同一输入，输出之间的差距。

KL散度越小，意味着两个分布之间的相似程度越大。

## PPO-penalty（近端策略优化惩罚）
指的是KL散度项的惩罚大小。其受到KL散度的值以及$ \beta $所决定。PPO的论文里采用的是自适应KL散度（adaptive KL divergence），即设定KL散度在一个范围（min，max）之间：

+ 如果超过max，则认为两个分布差距特别大，需要加重惩罚，$ \beta $需要`**增大**`；
+ 如果小于min，则认为两个分布特别相似，不需要太大惩罚，$ \beta $需要`**减小**`。

## 近端优化裁剪
不直接使用KL散度进行计算，而是使用一个裁剪函数进行。其最大化的目标函数为：

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1767055185896-be20b039-d0cb-43d6-a453-5c7edb73650f.png)

+ 操作符（operator）min 是在第一项与第二项里面选择比较小的项。
+ 第二项前面有一个裁剪（clip）函数，裁剪函数是指，在括号里面有3项，如果第一项小于第二项，那就输出$ 1-ε $；第一项如果大于第三项，那就输出$ 1+ε $
+ $ ε $是超级参数，可以设置为0.1或0.2；

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1767055330598-9d8cda06-d09c-4b9d-be25-d0f3c5e44dff.png)

裁剪函数

# 小结
通常PPO的表现是不错的，即使不是最好，而是第二好。

# 问题
## 基于online policy的PG算法有什么改进之处？
基于online policy需要等agent和env完成交互后才能进行更新。效率特别低，而且每一个回合仅能更新一次，更新完之后要花时间重采样。

使用off-line policy的方法，可以使用同一组数据，对策略进行多次更新，减少了采样时间。

## 重要性采样需要注意的问题？
分布相差不可以太大，否则最后的方差会很大，效果不好。

## KL散度是什么？
衡量两个分布的相似程度，具体而言，指的是同一个输入，其输出的差异。





