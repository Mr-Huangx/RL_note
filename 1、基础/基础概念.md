# 概述
**Q**：what is reinforcement learning（RL）？

**强化学习（reinforcement learning，RL**）讨论的问题是agent怎么不确定的environment中最大化它能获得的奖励。



# 序列决策（sequential decision making）
**agent**把它的动作输出给**environment**，**environment**取得这个动作后会进行下一步，把下一步的**observation**与这个动作带来的**reward**返还给智能体。这样的交互会产生很多**observation**，**agent**的目的是从这些**observation**之中学到能最大化**rewards**的策略。



## 奖励（reward）
**奖励reward**是由**environment**给的一种标量的反馈信号，衡量agent在某一步采取某个策略的表现如何。



## 序列决策
在agent和enviroment进行交互的过程中，agent会获得很多observation。针对每一个observation，只能提会采取一个action，也会获得一个reward。循环往复，所以历史是observation、action、reward的序列。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1766537405577-bcac2598-dc3d-444c-b4f1-e878d165c8f3.png)

因此，agent在当前observation采取的动作是依赖于前面得到的历史，我们可以把它看作关于这个历史的函数：

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1766537458694-734669fc-9f2c-4733-b00d-73bda3893fdd.png)

# 动作空间
agent可以采取的动作集合。

例如，走迷宫机器人如果只有往东、往南、往西、往北这 4 种移动方式，则其动作空间为离散动作空 间；如果机器人可以向 360 度中的任意角度进行移动，则其动作空间为连续动作空间。



# 策略（policy）
即输入agent的state，然后输出对应采取的动作。策略分为：随机性策略**（stochastic policy）**和确定性策略**（deterministic policy）**。

**随机性策略****（stochastic policy）：**$ \pi $函数，即$ \pi(a|s)=p(a_t=a|s_t=s) $。即agent输入对应所在的state，$ \pi
 $函数输出采取动作空间中所有action的概率，然后随机采样，获取对应的action。

**确定性策略（deterministic policy）**：同样可以等同于$ \pi $函数，只是其输出的概率中，只有一个action的概率大于0，且等于1，其他的action的概率均为0；



## 价值函数（state value）
`**state value**`，表示对未来奖励的预测，我们用它评价当前state的好坏。计算state value的公式如下:

$ V_\pi (s)=E_\pi [G_t|s_t=s]=E_\pi[\sum_{k=0}^\infty \gamma ^kr_{t+k+1}|s_t=s] $

state value的计算是依赖于policy$ \pi $,因此，其也反映了policy的好坏。

`**action value**`，Q函数。定义如下：

$ Q_\pi(s,a) = E_\pi[G_t|s_t=s,a_t=a]=E_\pi[\sum_{k=0}^\infty \gamma ^k r_{r+k+1}|s_t=s,a_t=a] $

仔细观察，可以看出，**action value**指的是在某个**state**下采取某个`action`能获取到的`**discounted return**`。而**state value**是对当前状态下所有**action**的**action value**取了**均值**。



## Model（模型）
模型决定的是下一步的状态。下一步的state由两部分组成：当前state+当前采取的action。它由状态转移概率和奖励函数两个部分组成。状态转移概率即：

$ p^a_{ss'}=p(s_{t+1}=s'|s_t=s,a_t=a) $

奖励函数（reward function）是指我们在当前状态采取了某个动作，可以得到多大的奖励,公式如下：

$ R(s,a)=E[r_{t+1}|s_t=s,a_t=a] $

当我们有了策略、价值函数和模型3个组成部分后，就形成了一个**马尔可夫决策过程（Markov decision process）。**

****

# 例子
借用《蘑菇书》的例子，来讲解下强化学习的两种类型，以便构建整个RL的世界观：如下图所示，要求智能体从起点（start）开始，然后到达终点（goal）的位置。每走一步，我们就会得到 −1 的奖励。我们可以采取的动作是往上、下、左、右走。我们用现在智能体所在的位置来描述当前状态。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1766582876432-434fa28e-fa5c-473e-8c8a-0db01281a716.png)

**迷宫例子**

我们可以用不同的强化学习方法来解这个环境。 如果我们采取`**基于策略**`的强化学习（policy-based RL）方法，当学习好了这个环境后，在每一个状态，我们都会得到一个最佳的动作。如图 1.17 所示，比如我们现在在起点位置，我们知道最佳动作是往右走；在第二格的时候，得到的最佳动作是往上走；第三格是往右走......通过最佳的策略，我们可以最快地到达终点。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1766582908776-ec931b6b-9c65-4fc8-989e-b9979f4b4d7a.png)

**policy based model**

如果换成`**基于价值**`的**强化学习**（`**value-based RL**`）方法，利用价值函数作为导向，我们就会得到另外一种表征，每一个状态会返回一个价值。如下图所示，比如我们在起点位置的时候，价值是−16，因为我们最快可以 16 步到达终点。因为每走一步会减1，所以这里的价值是−16。 当我们快接近终点的时候，这个数字变得越来越大。在拐角的时候，比如现在在第二格，价值是−15，智能体会看上、下两格，它看到上面格子的价值变大了，变成−14 了，下面格子的价值是−16，那么智能体就会采取一个往上走的动作。所以通过学习的价值的不同，我们可以抽取出现在最佳的策略。

![](https://cdn.nlark.com/yuque/0/2025/png/35251293/1766582970644-2ef8be95-5480-4aff-83e9-2e73233fd059.png)

**value based model**



# 强化学习agent的类型
## value based & model based
**基于价值的智能体（value-based agent）**显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。**基于策略的智能体（policy-based agent）**直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。两者相结合就又了**actor-critic agent**。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。



## model free & model based
**有模型（model-based）**强化学习智能体通过学习状态的转移来采取动作。 **免模型（model-free**）强化学习智能体没有去直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数和策略函数进行决策。免模型强化学习智能体的模型里面**没有环境转移**的模型。

我们可以用马尔可夫决策过程来定义强化学习任务，并将其表示为四元组 $ <S,A,P,R> $，即`**状态集合**`、`**动作集合**`、`**状态转移函数**`和`**奖励函数**`。如果这个四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则智能体可以对`**真实环境**`进行建模，构建一个`**虚拟世界**`来模拟真实环境中的状态和交互反应。 具体来说，当智能体知道状态转移函数$ p^a_{ss'}=p(s_{t+1}=s'|s_t=s,a_t=a) $ 和奖励函数 $ R(s,a)=E[r_{t+1}|s_t=s,a_t=a] $ 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为有`**模型强化学习**`。

`**免模型强化学习**`没有对真实环境进行建模，而是通过不断的在真实世界中`**试错**`，获取反馈信息，来更新动作策略，这样反复迭代直到学习最优策略。



# 问题
## 可以用一句话谈一下你对于强化学习的认识吗？
智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大。

## 你认为强化学习、监督学习和无监督学习三者有什么区别呢
强化学习具有延迟奖励的特性。

强化学习的数据多为序列数据，样本之间具有强相关性。

强化学习的目标是使agent获得的奖励最大。

## 你认为强化学习的使用场景有哪些呢？
多序列决策问题。或者说是对应模型未知，需要通过学习逼近真实模型的问题。

## 请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？
深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。

## 你了解model based模型和model free模型吗？两者具体有什么区别呢？
根据是否对真实环境进行建模分为有模型和无模型，具体而言就是针对马尔可夫属性集合中的状态转移函数和奖励函数是否已知进行建模。



